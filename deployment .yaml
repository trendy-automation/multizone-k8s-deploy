---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: scaling-role
  namespace: tst-namespace
rules:
  - apiGroups: ["apps"]
    resources: ["deployments/scale""]
    verbs: ["patch"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:

  name: salme-pods
  namespace: tst-namespace
subjects:
- kind: ServiceAccount
  name: deployment-scaler
  namespace: tst-namespace
roleRef:
  kind: Role
  name: scaling-role
  apiGroup: rbac.authorization.k8s.io

---
kind: ServiceAccount
metada:
  name: deployment-scaler

# <- приложение имеет дневной цикл по нагрузке – ночью запросов на порядки меньше, пик – днём

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: scale-up
spec:
  schedule: "* 10 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccount: deployment-scaler
          ...
            command: kubectl scale --replicas=4 deployment webapp-deployment

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: scale-down
spec:
  schedule: "* 23 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccount: deployment-scaler
          ...
            command: kubectl scale --replicas=2 deployment webapp-deployment

---

apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-deployment
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp-deployment
  minReplicas: 2
  maxReplicas: 8
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 80

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-deployment
  namespace: tst-namespace
  labels:
    app: webapp
spec:
  # <- по результатам нагрузочного теста известно, что 4 пода справляются с пиковой нагрузкой
  # <- хотим максимально отказоустойчивый deployment
  # <- хотим минимального потребления ресурсов от этого deployment’а
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      # <- у нас мультизональный кластер (три зоны), в котором пять нод
      affinity:
        # 1. Хотим распределять по нодам в рамках одной AZ так, чтобы Pod-ы старались попасть на разные ноды
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - webapp
            topologyKey: "kubernetes.io/webapphostname"
        # 2. Хотим распределять Pod-ы по AZ равномерно
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - webapp
              topologyKey: failure-domain.beta.kubernetes.io/zone
          
      containers:
      - name: nginx
        image: nginx:1.23.4

        # на первые запросы приложению требуется значительно больше ресурсов CPU
        # в дальнейшем потребление ровное в районе 0.1 CPU. По памяти всегда “ровно” в районе 128M memory
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "128Mi"
            cpu: "1000m"

        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
            httpHeaders:
            - name: Custom-Header
              value: Awesome
          initialDelaySeconds: 3
          periodSeconds: 2

        readinessProbe:
          exec:
            command:
            - cat
            - /tmp/healthy
          initialDelaySeconds: 5
          periodSeconds: 5
          
        startupProbe:
          httpGet:
            path: /healthz
            port: liveness-port
          failureThreshold: 30
          periodSeconds: 10  # <- приложение требует около 5-10 секунд для инициализации